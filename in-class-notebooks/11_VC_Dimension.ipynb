{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11: VC Dimension\n",
    "***\n",
    "\n",
    "<img src=\"figs/cogs.jpg\",width=1100,height=50>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**: *If the math type-setting looks funny, scroll down and shift-enter the single sell under Helper Functions*\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Problem 1: VC Dimension of Hyperplanes Through the Origin \n",
    "***\n",
    "\n",
    "Consider the class of linear hyperplanes in 2D that pass through the origin where each hypothesis can be defined to classify points above/left or below/right of the hyperplane as positive. \n",
    "\n",
    "<img src=\"figs/planes2.png\",width=800,height=50>\n",
    "\n",
    "**Q**: State and prove the VC Dimension of this hypothesis class. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 2: VC Dimension of Union of Two Intervals \n",
    "***\n",
    "\n",
    "**Q**: State and prove the VC Dimension of unions of two intervals on the real line.  A classifier  in this class associated with the union $[a, b] \\cup [c, d]$ classifies a point $x$ as positive if $x \\in [a,b] \\cup [c,d]$ and negative otherwise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 3: VC Dimension of Linear Classifiers in $\\mathbb{R}^D$\n",
    "***\n",
    "\n",
    "It was stated in the lecture that the VC Dimension of a linear classifier with data in $\\mathbb{R}^D$ is $D+1$.  In this problem you will prove this result, first by proving that there exists a set of $D+1$ points that can be shattered, and then by proving that there does not exist a set of $D+2$ points that can be shattered. \n",
    "\n",
    "The general form of our classifier is $h_{\\bf w}({\\bf x}) = I_{{\\bf w}^T{\\bf x} \\geq 0}$, i.e. $h_{\\bf w}$ classifies an example ${\\bf x}$ as positive if ${\\bf w}^T{\\bf x} \\geq 0$ and negative otherwise. Note that here we have (as usual) prepended the ${\\bf x}$ vector with a $1$ corresponding to the bias term.  Thus both ${\\bf w}$ and ${\\bf x}$ live in $\\mathbb{R}^{D+1}$.  We'll assume that our labels are $y_i \\in \\{1,-1\\}$ corresponding to ${\\bf w}^T{\\bf x} \\geq 0$ and ${\\bf w}^T{\\bf x} < 0$, respectively. \n",
    "\n",
    "**Q**: In the first step, we want to show that there exists $D+1$ vectors ${\\bf x}_i$ such that $\\textrm{sign}({\\bf w}^T{\\bf x_i}) = y_i$ for any labeling of $y_i$.  Argue that we can test the suitability of a proposed set of $D+1$ vectors by an equivalent matrix equation of the form $X{\\bf w} = {\\bf y}$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Find a suitable set of points ${\\bf x}_1, \\ldots, {\\bf x}_{D+1}$ that can be shattered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Show that there does not exist a set of $D+2$ points in $\\mathbb{R}^D$ that a linear classifier can shatter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "<br><br><br><br><br>\n",
    "<br><br><br><br><br>\n",
    "\n",
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Problem 1: VC Dimension of Hyperplanes Through the Origin \n",
    "***\n",
    "\n",
    "Consider the class of linear hyperplanes in 2D that pass through the origin where each hypothesis can be defined to classify points above/left or below/right of the hyperplane as positive. \n",
    "\n",
    "<img src=\"figs/planes2.png\",width=800,height=50>\n",
    "\n",
    "**Q**: State and prove the VC Dimension of this hypothesis class. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 2: VC Dimension of Union of Two Intervals \n",
    "***\n",
    "\n",
    "**Q**: State and prove the VC Dimension of unions of two intervals on the real line.  A classifier  in this class associated with the union $[a, b] \\cup [c, d]$ classifies a point $x$ as positive if $x \\in [a,b] \\cup [c,d]$ and negative otherwise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: The VC Dimension of this hypothesis class is $4$.  To prove this, we need to show the shattering of a particular set of $4$ points, and then prove that no set of $5$ points can be shattered.  \n",
    "\n",
    "Let $x_1, \\ldots, x_4$ be points positioned on consecutive integers on the real line.  There are $2^4=16$ possible dichotomies on $x_1, \\ldots, x_4$.  We need to show that we can accurately classify all of them. \n",
    "\n",
    "$$\n",
    "\\begin{array}{|cccc|}\n",
    "\\hline \n",
    "x_1 & x_2 & x_3 & x_4 & \\\\ \n",
    "\\hline \n",
    "[+ & + & + & +] & [] \\\\\n",
    "[+ & + & +] & - & []\\\\\n",
    "[+ & +] & - & [+] & \\\\\n",
    "[+ & +] & - & - & [] \\\\\n",
    "[+] & - & [+ & +] & \\\\\n",
    "[+] & - & [+] & - & \\\\\n",
    "[+] & - & - & [+] & \\\\\n",
    "[+] & - & - & - & [] \\\\\n",
    "- & [+ & + & +] & []\\\\\n",
    "- & [+ & +] & - & []\\\\\n",
    "- & [+] & - & [+] & \\\\\n",
    "- & [+] & - & - & []\\\\\n",
    "- & - & [+ & +] & []\\\\\n",
    "- & - & [+] & - & []\\\\\n",
    "- & - & - & [+] & []\\\\\n",
    "- & - & - & - & [][]\\\\\n",
    "& & & &  \\\\\n",
    "\\hline \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "So we've proved that $\\textrm{VCdim}(H) \\geq 4$.  Next we need to show that $\\textrm{VCdim}(H) < 5$ by showing that **no** set of 5 points can be shattered by the union of a pair of intervals.  First, we can assume that none of the 5 points can lie on top of each other, since any labeling that assigned opposite labels to those two points could not be classified exactly.  Thus we can assume that the 5 points are distinct, and without loss of generality, order them as $x_1 < x_2 < x_3 < x_4 < x_5$.  Now consider the labeling $y_1 = +1, y_2 = -1, y_3 = +1, y_4 = -1,$ and $y_5 = +1$.  Now note that any interval that contains multiple points will be an incorrect labeling since the points alternate in label.  This then requires that we put intervals around individual positively labeled points, but there are three of them and we only have two intervals to burn, thus this labeling is impossible.   \n",
    "\n",
    "We've shown that $\\textrm{VCdim}(H) \\geq 4$ and  $\\textrm{VCdim}(H) < 5$, thus $\\textrm{VCdim}(H) = 4$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 3: VC Dimension of Linear Classifiers in $\\mathbb{R}^D$\n",
    "***\n",
    "\n",
    "It was stated in the lecture that the VC Dimension of a linear classifier with data in $\\mathbb{R}^D$ is $D+1$.  In this problem you will prove this result, first by proving that there exists a set of $D+1$ points that can be shattered, and then by proving that there does not exist a set of $D+2$ points that can be shattered. \n",
    "\n",
    "The general form of our classifier is $h_{\\bf w}({\\bf x}) = I_{{\\bf w}^T{\\bf x} \\geq 0}$, i.e. $h_{\\bf w}$ classifies an example ${\\bf x}$ as positive if ${\\bf w}^T{\\bf x} \\geq 0$ and negative otherwise. Note that here we have (as usual) prepended the ${\\bf x}$ vector with a $1$ corresponding to the bias term.  Thus both ${\\bf w}$ and ${\\bf x}$ live in $\\mathbb{R}^{D+1}$.  We'll assume that our labels are $y_i \\in \\{1,-1\\}$ corresponding to ${\\bf w}^T{\\bf x} \\geq 0$ and ${\\bf w}^T{\\bf x} < 0$, respectively. \n",
    "\n",
    "**Q**: In the first step, we want to show that there exists $D+1$ vectors ${\\bf x}_i$ such that $\\textrm{sign}({\\bf w}^T{\\bf x_i}) = y_i$ for any labeling of $y_i$.  Argue that we can test the suitability of a proposed set of $D+1$ vectors by an equivalent matrix equation of the form $X{\\bf w} = {\\bf y}$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Let's make our lives harder and instead of enforcing $\\textrm{sign}({\\bf w}^T{\\bf x_i}) = y_i$ we actually enforce ${\\bf w}^T{\\bf x_i} = y_i$ for $i=1,\\ldots, D+1$. Writing this system of equations out for each $i$ we have\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "{\\bf x}_1^T{\\bf w} &=& y_1 \\\\\n",
    "{\\bf x}_2^T{\\bf w} &=& y_2 \\\\\n",
    "{\\bf x}_3^T{\\bf w} &=& y_3 \\\\\n",
    "\\vdots && \\vdots \\\\\n",
    "{\\bf x}_{D+1}^T{\\bf w} &=& y_{D+1} \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Notice then that we can write this as \n",
    "\n",
    "$$\n",
    "X{\\bf w} = \\left[\n",
    "\\begin{array}{ccc}\n",
    "- & {\\bf x}_1^T & - \\\\\n",
    "- & {\\bf x}_2^T & - \\\\\n",
    "- & {\\bf x}_3^T & - \\\\\n",
    "&\\vdots& \\\\\n",
    "- & {\\bf x}_{D+1}^T & - \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "{\\bf w}\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \\\\\n",
    "\\vdots \\\\\n",
    "y_{D+1}\n",
    "\\end{array}\n",
    "\\right]\n",
    "= {\\bf y}\n",
    "$$\n",
    "\n",
    "So, we can find a set of $D+1$ points that are shatterable if we can find an associated matrix $X$ that is invertible.  Then the weight vector for a particular dichotomy encoded in ${\\bf y}$ is found simply by ${\\bf w} = X^{-1}{\\bf y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Find a suitable set of points ${\\bf x}_1, \\ldots, {\\bf x}_{D+1}$ that can be shattered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Instead of picking a set of points we'll equivalently populated the rows of $X$ such that $X$ is invertible.  First notice that we are not free to choose the first column of $X$ because it corresponds to the bias feature.  So we have at the beginning \n",
    "\n",
    "$$\n",
    "X = \\left[\n",
    "\\begin{array}{ccccc}\n",
    "1 & * & * & \\cdots & * \\\\\n",
    "1 & * & * & \\cdots & * \\\\\n",
    "\\vdots & \\vdots  & \\vdots  & & \\vdots  \\\\\n",
    "1 & * & * & \\cdots & * \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "There are many ways that we can choose the points such that $X$ is invertible.  One easy one is to make $X$ a lower-triangular matrix with nonzeros on the main diagonal.  A simple example is as follows \n",
    "\n",
    "$$\n",
    "X = \\left[\n",
    "\\begin{array}{ccccc}\n",
    "1 & 0 &   \\cdots & 0 & 0 \\\\\n",
    "1 & 1 &   \\cdots & 0 & 0 \\\\\n",
    "\\vdots &   & \\ddots  & &  \\vdots  \\\\\n",
    "1 & 0 &   \\cdots & 1 & 0 \\\\\n",
    "1 & 0 &   \\cdots & 0 & 1\\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Note that this corresponds to one training point sitting at the origin, and $D$ training points positioned out 1 unit along each of the $D$ axes. \n",
    "\n",
    "Since we've found a particular set of $D+1$ points that can be shattered, we know that $\\textrm{VCdim}(h_{\\bf w}) \\geq D+1$.  In the next step we'll prove that $\\textrm{VCdim}(h_{\\bf w}) \\leq D+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Show that there does not exist a set of $D+2$ points in $\\mathbb{R}^D$ that a linear classifier can shatter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: There are a couple of ways to do this, depending on how much you like or dislike linear algebra.  \n",
    "\n",
    "**Linear Algebra Heavy**: Suppose that there are $D+2$ points that will work.  Then, we should be able to find the associated weight vector for a given dichotomy ${\\bf y} \\in \\mathbb{R}^{D+2}$ by solving a similar linear system of the form $X{\\bf w} = {\\bf y}$ where this time $X$ is $(D+2) \\times (D+1)$.  Note that this is an overdetermined system and is only solvable if ${\\bf y}$ is in the range of ${\\bf X}$.  Since ${\\bf X}$ only has $(D+1)$ columns the dimension of its range is at most $(D+1)$, but the dimension of the space that all possible dichotomies for ${\\bf y}$ live in has dimension $(D+2)$, thus there must be at least one that is missed.   \n",
    "\n",
    "**Linear Algebra Light**: Let ${\\bf x}_1, {\\bf x}_2, \\ldots, {\\bf x}_{D+2}$ be $(D+2)$ arbitrary vectors in $\\mathbb{R}^{D+1}$.  Since we have more vectors than dimensions, at least one of those vectors must be linearly dependent on the others.  Thus, for some $j$ we can write\n",
    "\n",
    "$$\n",
    "{\\bf x}_j = \\sum_{i \\neq j} \\alpha_i {\\bf x}_i \n",
    "$$\n",
    "\n",
    "where at least some of the $\\alpha_i$'s are nonzero.  Now, create a dichotomy where $y_i = \\textrm{sign}(\\alpha_i)$ if $\\alpha_i \\neq 0$ and let $y_j = -1$.  A linear classifier can not solve this dichotomy.  To see this, note that \n",
    "\n",
    "$$\n",
    "-1 = {\\bf w}^T{\\bf x}_j = {\\bf w}^T\\sum_{i \\neq j} \\alpha_i {\\bf x}_i \n",
    "= \\sum_{i \\neq j} \\alpha_i {\\bf w}^T{\\bf x}_i \n",
    "= \\sum_{i \\neq j} \\alpha_i  \\textrm{sign}(\\alpha_i) > 0 \n",
    "$$\n",
    "\n",
    "This is a contradiction, thus there is no set of $(D+2)$ points that can be shattered. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
