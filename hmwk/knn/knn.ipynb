{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Homework Solutions\n",
    "***\n",
    "**Name**: $<$insert name here$>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday January 26th**. Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "\n",
    "In this homework you'll implement a K-Nearest Neighbor framework to take an image of a handwritten digit and predict which digit it corresponds to.  \n",
    "\n",
    "<br>\n",
    "\n",
    "![Samples of Handwritten Digits](wide_mnist.png \"MNIST Digits\")\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "This homework is designed to be pretty easy. If you're spending a ton of time on this assignment, then you are either:\n",
    "\n",
    "- not prepared to take this course (i.e., if you're struggling with Python)\n",
    "- seriously over-thinking the assignment\n",
    "- trying to implement too much of KNN from scratch\n",
    "\n",
    "\n",
    "Most of this assignment will be done by calling libraries that are already implemented for you. If you are implementing $n$-dimensional search or your own distance metrics, you are generating extra work for yourself and making yourself vulnerable to errors. \n",
    "\n",
    "Here are the rules: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function or class API **do not** change it.\n",
    "- Do not change the location of the data or data directory.  Use only relative paths to access the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Problem 1\n",
    "***\n",
    "\n",
    "The class below will load and store the MNIST data.  Load the data and then report: \n",
    "- The number of examples in the training set \n",
    "- The number of examples in the test set \n",
    "- The number of pixels in each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Numbers:\n",
    "    \"\"\"\n",
    "    Class to store MNIST data\n",
    "    \"\"\"\n",
    "    def __init__(self, location):\n",
    "\n",
    "        import pickle, gzip\n",
    "\n",
    "        # load data from file \n",
    "        f = gzip.open(location, 'rb')\n",
    "        train_set, valid_set, test_set = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "        # store for use later  \n",
    "        self.train_x, self.train_y = train_set\n",
    "        self.test_x, self.test_y = valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = Numbers(\"../data/mnist.pklz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 2\n",
    "***\n",
    "\n",
    "The class below will perform K-Nearest Neighbor classification on our handwritten digit data. Your tasks are as follows:   \n",
    "\n",
    "1. Modify the `label_counts` function to return a dictionary of frequencies corresponding to each label in the training set. \n",
    "1. Modify the `majority` function so that it returns the _label_ that appears most frequently in the $K$-nearest neighbors of the query point.  In the case that the maximum frequency occurs for two or more labels, return the one that appears most frequently in the entire training set. In the case that there is still a tie, break the tie in any way that you choose. \n",
    "1. Modify the `classify` function so that it finds the _indices_ of the $K$ closest training examples to the query point and then calls the `majority` function to return the predicted label. Almost all of the heavy lifting here will be done by the BallTree object from `sklearn.neighbors`, so you'll want to start out by reading the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html).  \n",
    "1. Modify the `confusion_matrix` function to classify examples and record the results in a confusion matrix. You should construct the confusion matrix on your own.  Don't call any additional functions from sklearn to do it for you.\n",
    "\n",
    "The class Knearest also implements an `accuracy` function which you will use in **Problem 3**.  You should not have to modify this function. \n",
    "\n",
    "We've given you unit tests down below based on the simple example worked out in lecture.  At first your code will fail all of them.  Do not move on to **Problem 3** until your code passes all of the unit tests. In addition, passing the unit tests does not guarantee that your implementation is robust and that you'll earn full points on this problem.  You should be designing your own additional tests as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Knearest:\n",
    "    \"\"\"\n",
    "    kNN classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, k=5):\n",
    "        \"\"\"\n",
    "        Creates a kNN instance\n",
    "\n",
    "        :param x: Training data input\n",
    "        :param y: Training data output\n",
    "        :param k: The number of nearest points to consider in classification\n",
    "        \"\"\"\n",
    "        \n",
    "        from sklearn.neighbors import BallTree\n",
    "\n",
    "        self._kdtree = BallTree(X)\n",
    "        self._y = y\n",
    "        self._k = k\n",
    "        self._counts = self.label_counts()\n",
    "        \n",
    "    def label_counts(self):\n",
    "        \"\"\"\n",
    "        Given the training labels, return a dictionary d where d[y] is  \n",
    "        the number of times that label y appears in the training set. \n",
    "        \"\"\"\n",
    "        return dict({1:0, -1:0})\n",
    "\n",
    "    def majority(self, neighbor_indices):\n",
    "        \"\"\"\n",
    "        Given the indices of training examples, return the majority label. Break ties \n",
    "        by choosing the tied label that appears most often in the training data. \n",
    "\n",
    "        :param neighbor_indices: The indices of the k nearest neighbors\n",
    "        \"\"\"\n",
    "        assert len(neighbor_indices) == self._k, \"Did not get k neighbor indices\"\n",
    "        \n",
    "        return self._y[neighbor_indices[0]]\n",
    "    \n",
    "\n",
    "    def classify(self, example):\n",
    "        \"\"\"\n",
    "        Given an example, return the predicted label. \n",
    "\n",
    "        :param example: A representation of an example in the same\n",
    "        format as a row of the training data\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.majority([np.random.randint(0,10) for ii in range(self._k)])\n",
    "\n",
    "\n",
    "    def confusion_matrix(self, test_x, test_y):\n",
    "        \"\"\"\n",
    "        Given a matrix of test examples and labels, compute the confusion\n",
    "        matrix for the current classifier.  Should return a 2-dimensional\n",
    "        numpy array of ints, C, where C[ii,jj] is the number of times an \n",
    "        example with true label ii was labeled as jj.\n",
    "\n",
    "        :param test_x: test data \n",
    "        :param test_y: true test labels \n",
    "        \"\"\"\n",
    "        \n",
    "        C = np.zeros((10,10), dtype=int)\n",
    "        for xx, yy in zip(test_x, test_y):\n",
    "            pass \n",
    "        \n",
    "        return C \n",
    "            \n",
    "    @staticmethod\n",
    "    def accuracy(C):\n",
    "        \"\"\"\n",
    "        Given a confusion matrix C, compute the accuracy of the underlying classifier.\n",
    "        \n",
    "        :param C: a confusion matrix \n",
    "        \"\"\"\n",
    "        \n",
    "        return np.sum(C.diagonal()) / C.sum()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the unit tests.  You don't need to modify them.  Simply execute the cell and observe the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestKnn(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.x = np.array([[2, 0], [4, 1], [6, 0], [1, 4], [2, 4], [2, 5], [4, 4], [0, 2], [3, 2], [4, 2], [5, 2], [5, 5]])\n",
    "        self.y = np.array([+1, +1, +1, +1, +1, +1, +1, -1, -1, -1, -1, -1])\n",
    "        self.knn = {}\n",
    "        for ii in [1, 2, 3]:\n",
    "            self.knn[ii] = Knearest(self.x, self.y, ii)\n",
    "\n",
    "        self.queries = np.array([[1, 5], [0, 3], [6, 4]])\n",
    "        \n",
    "    def test0(self):\n",
    "        \"\"\"\n",
    "        Test the label counter \n",
    "        \"\"\"\n",
    "        self.assertEqual(self.knn[1]._counts[-1], 5)\n",
    "        self.assertEqual(self.knn[1]._counts[1], 7)\n",
    "\n",
    "    def test1(self):\n",
    "        \"\"\"\n",
    "        Test 1NN\n",
    "        \"\"\"\n",
    "        self.assertEqual(self.knn[1].classify(self.queries[0]),  1)\n",
    "        self.assertEqual(self.knn[1].classify(self.queries[1]), -1)\n",
    "        self.assertEqual(self.knn[1].classify(self.queries[2]), -1)\n",
    "\n",
    "    def test2(self):\n",
    "        \"\"\"\n",
    "        Test 2NN\n",
    "        \"\"\"\n",
    "        self.assertEqual(self.knn[2].classify(self.queries[0]),  1)\n",
    "        self.assertEqual(self.knn[2].classify(self.queries[1]),  1)\n",
    "        self.assertEqual(self.knn[2].classify(self.queries[2]),  1)\n",
    "\n",
    "    def test3(self):\n",
    "        \"\"\"\n",
    "        Test 3NN\n",
    "        \"\"\"\n",
    "        self.assertEqual(self.knn[3].classify(self.queries[0]),  1)\n",
    "        self.assertEqual(self.knn[3].classify(self.queries[1]),  1)\n",
    "        self.assertEqual(self.knn[3].classify(self.queries[2]), -1)\n",
    "        \n",
    "tests = TestKnn()\n",
    "tests_to_run = unittest.TestLoader().loadTestsFromModule(tests)\n",
    "unittest.TextTestRunner().run(tests_to_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Problem 3\n",
    "***\n",
    "\n",
    "In this problem you'll explore the performance of the classifier you've written.  A word of advice: don't use the entire training set, especially at first.  We'll be using this dataset again later on with techniques that scale better.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Explore the relationship between the number of training examples and accuracy on the test set. Comment on your findings and support your observations with some kind of graphic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Explore the relationship between the number of nearest neighbors and accuracy on the test set. Comment on your findings and support your observations with some kind of graphic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Which numbers get confused with each other the most easily?  Use the confusion_matrix function that you wrote above to support your conclusion.  Then use the `view_digit` function given below to plot a few examples of misclassified digits and discuss possible reasons for the misclassifications.  (Investigating misclassified examples is called **error analysis** and is an important step in the development of any classification technique).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_digit(example, label=None):\n",
    "    if label: print(\"true label: {:d}\".format(label))\n",
    "    plt.imshow(example.reshape(28,28), cmap='gray');\n",
    "    \n",
    "view_digit(data.train_x[0,:], data.train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
