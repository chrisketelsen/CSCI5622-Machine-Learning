{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Homework \n",
    "***\n",
    "**Name**: $<$insert name here$>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday April 13th**. Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "In this homework you'll implement the AdaBoost classification framework to do handwritten digit recognition. Your implementation should be based on the description of AdaBoost given in the lecture slides.\n",
    "\n",
    "<br>\n",
    "\n",
    "![digits](mnist.png \"mnist data\")\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Here are the rules: \n",
    "\n",
    "- Do **NOT** use sklearn's implementation of Adaboost.  You may however use sklearn's implementation of decisions trees. \n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function or class API **do not** change it.\n",
    "- Do not change the location of the data or data directory.  Use only relative paths to access the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:16.624747Z",
     "start_time": "2018-04-02T09:58:15.916585Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone \n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Problem 1\n",
    "***\n",
    "\n",
    "Since we'll be working with binary classifiers, we'll look at the subset of the MNIST data pertaining to handwritten three's and eights. Note that we'll also be using a lower-res version of the MNIST data used in the KNN homework. The class below will load, parse, and store the subset of the. Load the data and then report: \n",
    "\n",
    "- The number of examples in the training set \n",
    "- The number of examples in the validation set \n",
    "- The number of pixels in each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:18.840016Z",
     "start_time": "2018-04-02T09:58:18.805889Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ThreesAndEights:\n",
    "    \"\"\"\n",
    "    Class to store MNIST data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, location):\n",
    "\n",
    "        import pickle, gzip\n",
    "\n",
    "        # Load the dataset\n",
    "        f = gzip.open(location, 'rb')\n",
    "\n",
    "        # Split the data set \n",
    "        X_train, y_train, X_valid, y_valid = pickle.load(f)\n",
    "\n",
    "        # Extract only 3's and 8's for training set \n",
    "        self.X_train = X_train[np.logical_or( y_train==3, y_train == 8), :]\n",
    "        self.y_train = y_train[np.logical_or( y_train==3, y_train == 8)]\n",
    "        self.y_train = np.array([1 if y == 8 else -1 for y in self.y_train])\n",
    "        \n",
    "        # Shuffle the training data \n",
    "        shuff = np.arange(self.X_train.shape[0])\n",
    "        np.random.shuffle(shuff)\n",
    "        self.X_train = self.X_train[shuff,:]\n",
    "        self.y_train = self.y_train[shuff]\n",
    "\n",
    "        # Extract only 3's and 8's for validation set \n",
    "        self.X_valid = X_valid[np.logical_or( y_valid==3, y_valid == 8), :]\n",
    "        self.y_valid = y_valid[np.logical_or( y_valid==3, y_valid == 8)]\n",
    "        self.y_valid = np.array([1 if y == 8 else -1 for y in self.y_valid])\n",
    "        \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:19.740678Z",
     "start_time": "2018-04-02T09:58:19.609721Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ThreesAndEights(\"../data/mnist21x21_3789.pklz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 2: Implementing AdaBoost  \n",
    "***\n",
    "\n",
    "We've given you a skeleton of the class `AdaBoost` below which will train a classifier based on boosted shallow decision trees as implemented by sklearn. Take a look at the class skeleton first so that you understand the underlying organization and data structures that we'll be using.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:23.101470Z",
     "start_time": "2018-04-02T09:58:23.016833Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_learners=20, base=DecisionTreeClassifier(max_depth=1), random_state=1234):\n",
    "        \"\"\"\n",
    "        Create a new adaboost classifier.\n",
    "        \n",
    "        Args:\n",
    "            N (int, optional): Number of weak learners in classifier.\n",
    "            base (BaseEstimator, optional): Your general weak learner \n",
    "            random_state (int, optional): set random generator.  needed for unit testing. \n",
    "\n",
    "        Attributes:\n",
    "            base (estimator): Your general weak learner \n",
    "            n_learners (int): Number of weak learners in classifier.\n",
    "            alpha (ndarray): Coefficients on weak learners. \n",
    "            learners (list): List of weak learner instances. \n",
    "        \"\"\"\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        self.n_learners = n_learners \n",
    "        self.base = base\n",
    "        self.alpha = np.zeros(self.n_learners)\n",
    "        self.learners = []\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train AdaBoost classifier on data. Sets alphas and learners. \n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO \n",
    "\n",
    "        # Note: You can create and train a new instantiation \n",
    "        # of your sklearn decision tree as follows \n",
    "\n",
    "        w = np.ones(len(y_train))\n",
    "        h = clone(self.base)\n",
    "        h.fit(X_train, y_train, sample_weight=w)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Adaboost prediction for new data X.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            \n",
    "        Returns: \n",
    "            yhat (ndarray): [n_samples] ndarray of predicted labels {-1,1}\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO \n",
    "\n",
    "        return np.zeros(X.shape[0])\n",
    "        \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes prediction accuracy of classifier.  \n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            Prediction accuracy (between 0.0 and 1.0).\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # TODO \n",
    "\n",
    "        return 0.0\n",
    "        \n",
    "    \n",
    "    def staged_score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes the ensemble score after each iteration of boosting \n",
    "        for monitoring purposes, such as to determine the score on a \n",
    "        test set after each boost.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            scores (ndarary): [n_learners] ndarray of scores \n",
    "        \"\"\"\n",
    "\n",
    "        # TODO \n",
    "\n",
    "        return  np.zeros(self.n_learners)\n",
    "    \n",
    "    def staged_margin(self, x, y):\n",
    "        \"\"\"\n",
    "        Computes the staged margin after each iteration of boosting \n",
    "        for a single training example x and true label y\n",
    "        \n",
    "        Args:\n",
    "            x (ndarray): [n_features] ndarray of data \n",
    "            y (integer): an integer {-1,1} representing the true label of x \n",
    "            \n",
    "        Returns: \n",
    "            margins (ndarary): [n_learners] ndarray of margins \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO \n",
    "        \n",
    "        margins = np.zeros(self.n_learners)\n",
    "       \n",
    "        return margins\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the model we attempt to learn in AdaBoost is given by \n",
    "\n",
    "$$\n",
    "H({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^K\\alpha_k h_k({\\bf x}) \\right]\n",
    "$$\n",
    "\n",
    "where $h_k({\\bf x})$ is the $k^\\textrm{th}$ weak learner and $\\alpha_k$ is it's associated ensemble coefficient.  \n",
    "\n",
    "**Part A**: Implement the `fit` method to learn the sequence of weak learners $\\left\\{h_k({\\bf x})\\right\\}_{k=1}^K$ and corresponding coefficients $\\left\\{ \\alpha_k\\right\\}_{k=1}^K$. Note that you may use sklearn's implementation of DecisionTreeClassifier as your weak learner which allows you to pass as an optional parameter the weights associated with each training example.  An example of instantiating and training a single learner is given in the comments of the `fit` method.  \n",
    "\n",
    "When you think you're done, run the following unit tests which corresponds to the AdaBoost example given in the lecture slides. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:25.957948Z",
     "start_time": "2018-04-02T09:58:25.948471Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i tests.py \"part A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: After your `fit` method is working properly, implement the `predict` method to make predictions for unseen examples stored in a data matrix ${\\bf X}$.  \n",
    "\n",
    "**Note**: Remember that AdaBoost assumes that your predictions are of the form $y \\in \\{-1, 1\\}$. \n",
    "\n",
    "When you think you're done, run the following unit tests which corresponds to the AdaBoost example given in the lecture slides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:29.697855Z",
     "start_time": "2018-04-02T09:58:29.689876Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i tests.py \"part B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Next, implement the `score` method which takes in a matrix of examples ${\\bf X}$ and their associated true labels ${\\bf y}$, makes predictions, and returns the classification accuracy.   \n",
    "\n",
    "When you think you're done, run the following unit tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:32.327702Z",
     "start_time": "2018-04-02T09:58:32.320424Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i tests.py \"part C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Finally, implement the `staged_score` method to return an array of prediction accuracies after each iteration of the AdaBoost algorithm.  That is, the staged score array ${\\bf s}$ is defined such that ${\\bf s}_\\ell$ is the prediction accuracy using only the first $\\ell$ weak learners.  This function is primarily used as a diagnostic tool for analyzing the performance of your classifier during the training process.  \n",
    "\n",
    "**Note**: This method can be implemented in a very efficient or very **in**efficient matter.  Be sure to think about this a bit before diving in. \n",
    "\n",
    "\n",
    "When you think you're done, run the following unit tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:36.952096Z",
     "start_time": "2018-04-02T09:58:36.944919Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i tests.py \"part D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 3: AdaBoost for Handwritten Digit Recognition \n",
    "***\n",
    "\n",
    "Use your AdaBoost code with Sklearn's DecisionTreeClassifier as the base learner to distinguish $3$'s from $8$'s. \n",
    "Run $n=500$ boosting iterations with trees of depths 1, 2, and 3 (go deeper if you like) as the weak learner. For each weak learner, plot the training and validation error per boosting iteration (on the same set of axes). Compare and contrast the different weak learners. Which works the best? Do you see signs of overfitting? Do any of classifiers achieve nearly 100% accuracy on the training data? What happens to the accuracy on the validation data on further iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Problem 4: AdaBoost as a Margin-Maximizing Model \n",
    "***\n",
    "\n",
    "Despite the fact that we're making our model more complex with the addition of each weak learner, AdaBoost does not typically overfit the training data. The reason for this is that the model becomes more _confident_ with each boosting iteration. This _confidence_ can be interpreted mathematically as a margin. Recall that after $K$ iterations the algorithm terminates with the classifier \n",
    "\n",
    "$$\n",
    "H({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^K\\alpha_k h_k({\\bf x}) \\right]\n",
    "$$\n",
    "\n",
    "Similarly, we can define the intermediate classifier $H_\\ell$ by \n",
    "\n",
    "$$\n",
    "H_\\ell({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^\\ell\\alpha_k h_k({\\bf x}) \\right]\n",
    "$$\n",
    "\n",
    "where $\\ell \\leq K$. Note that in either case the model returns predictions of the form $y \\in \\{-1, 1\\}$ which does not give us any indication of the model's confidence in a prediction. Define the normalized coefficients $\\hat{\\alpha}_k$ as follows: \n",
    "\n",
    "$$\n",
    "\\hat{\\alpha}_k = \\dfrac{\\alpha_k}{\\sum_{t=1}^K \\alpha_k}\n",
    "$$\n",
    "\n",
    "Define the margin of a training example ${\\bf x}$ after $\\ell$ iterations as the sum of the normalized coefficients of weak learners that vote correctly minus the sum of the normalized coefficients of the weak learners that vote incorrectly: \n",
    "\n",
    "$$\n",
    "\\textrm{margin}_\\ell ({\\bf x}) = \\sum_{k=1:~h_k({\\bf x}) = y}^\\ell \\hat{\\alpha}_k - \\sum_{k=1:~h_k({\\bf x}) \\neq y}^\\ell \\hat{\\alpha}_k \n",
    "$$\n",
    "\n",
    "**Part A**: Briefly explain mathematically how $\\textrm{margin}_\\ell({\\bf x})$ can be interpreted as a margin.  **Hint**: You'll want to think back on what we meant by a _margin_ in our discussion of Support Vector Machines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the `staged_margin` method in the `AdaBoost` class above so that it computes the margin for a single training example ${\\bf x}$ after each boosting iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Find four **training** examples from the MNIST that meet the following criteria: \n",
    "\n",
    "- one $3$ that AdaBoost can classify easily  \n",
    "- one $8$ that AdaBoost can classify easily  \n",
    "- one $3$ that AdaBoost has difficulty with \n",
    "- one $8$ that AdaBoost has difficulty with \n",
    "\n",
    "Use the `view_digit` function given below to display the four examples that you found. \n",
    "\n",
    "**Advice**: Since AdaBoost will likely classify **all** training examples correctly given enough boosting iterations, you might try fitting an AdaBoost classifier with just a handful of boosting iterations and use it to identify examples of each desired type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:59:01.257421Z",
     "start_time": "2018-04-02T09:59:01.068539Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_digit(example, label=None):\n",
    "    if label: print(\"true label: {:d}\".format(label))\n",
    "    plt.imshow(example.reshape(21,21), cmap='gray');\n",
    "    \n",
    "view_digit(data.X_train[0,:], data.y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Using an AdaBoost classifier with at least $K=200$ depth-1 decision trees as the weak learners, plot the staged margin for each of the four examples that you found in **Part C** on the same set of axes. (Be sure to include a legend so we can tell which staged margin corresponds to which example).  Explain your results in terms of the margin of the classifier on each training examples.  More broadly, how the margin-maximizing property might allow AdaBoost to continue improving generalization even after the error on the training set reaches zero.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
